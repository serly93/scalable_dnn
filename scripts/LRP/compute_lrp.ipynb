{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.utils_lrp import *\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"  # Set the GPU card to use\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Or '3' for FATAL logs only\n",
    "np.set_printoptions(suppress=True)\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for gpu_instance in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(gpu_instance, True)\n",
    "import yaml\n",
    "# Set global parameters\n",
    "lookback = 3\n",
    "nr = 21\n",
    "city = 'Milan'\n",
    "nr2 = 2\n",
    "k = 4\n",
    "random_flag = False\n",
    "# Directories\n",
    "output_directory = '../../../../../oracle-data/serly/Scalable_dnn/PerBS/'\n",
    "cluster_directory = '../../../../../oracle-data/serly/Scalable_dnn/cluster/'\n",
    "model_deepcog_miMo = '../../../../../oracle-data/serly/Scalable_dnn/Trained_models/DeepCOG_miMo/'\n",
    "model_deepcog_original = '../../../../../oracle-data/serly/Scalable_dnn/Trained_models/DeepCOG_original/'\n",
    "main_dir = f'../../../../../oracle-data/serly/Scalable_dnn/MAE_plots/DeepCOG_miMo_vs_MiMo/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5370, 5470, 5170, 5369, 5450, 5169, 5250, 5469, 5550, 5253, 5168,\n",
       "       5270, 5266, 5265, 5150, 5366, 5070, 4763, 5466, 5067, 5069, 4851,\n",
       "       5551, 4867, 4969, 4156, 5350, 5365, 5364, 5357, 4850, 4155, 5066,\n",
       "       4868, 4764, 5451, 5167, 5065, 4752, 4866, 4852, 5554, 5356, 5153,\n",
       "       5052, 5569, 5555, 4164, 5570, 4970, 5456, 5465, 4154, 4966, 5553,\n",
       "       5552, 5652, 4550, 4165, 4952, 4157, 5556, 4151, 4163, 5565, 4152,\n",
       "       4951, 4669, 4066, 4770, 4865, 4150, 4651, 4153, 5653, 5455, 4864,\n",
       "       5464, 4470, 5051, 4070, 4652, 5050, 4067, 5351, 4162, 4161, 4065,\n",
       "       5650, 4069, 5165, 4062, 4068, 5651, 4670, 4551, 4751, 4170, 4061,\n",
       "       4063, 5564, 4159, 4053, 4059, 4060, 4869, 4052, 4357, 4668, 4169,\n",
       "       5457, 4965, 4250, 4358, 5768, 5755, 4570, 5751, 5764, 4058, 4166,\n",
       "       4251, 5452, 4064, 4158, 4750, 5763, 5669, 5770, 4054, 4259, 4258,\n",
       "       5666, 5454, 5756, 5668, 4469, 4051, 4765, 4252, 4369, 4368, 5670,\n",
       "       4453, 4950, 4370, 4050, 5557, 5166, 4452, 4270, 4255, 4666, 5750,\n",
       "       4667, 5654, 4657, 4552, 4253, 4254, 4367, 4450, 4057, 5850, 5656,\n",
       "       4263, 5352, 4264, 4354, 4350, 4168, 4055, 4262, 5354, 5355, 4056,\n",
       "       4569, 5852, 4360, 5655, 4167, 4564, 4468, 4870, 4351, 5665, 4269,\n",
       "       5657, 5353, 4565, 4451, 4265, 4563, 4769, 4562, 5853, 4353, 4566,\n",
       "       5463, 5664, 4352, 4567, 5558, 4454, 4663, 4568, 5663, 4366, 5563,\n",
       "       4665, 4467, 4268, 4361, 4266, 4768, 4664, 4766, 4267, 4465, 4466,\n",
       "       4365, 4464, 4767, 4364, 4463])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cells_dir = f'../../../../../oracle-data/serly/Scalable_dnn/LRP_scores/mimo_model/{city}/K_{k}/'\n",
    "ids = np.load(os.path.join(cells_dir, f'sorted_cells_{0}.npy'))\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_relevance(output, target_class=None):\n",
    "    \"\"\"\n",
    "    Initialize relevance at the output layer.\n",
    "    \"\"\"\n",
    "    relevance = np.zeros_like(output)\n",
    "    if target_class is not None:\n",
    "        relevance[:, target_class] = output[:, target_class]\n",
    "    else:\n",
    "        relevance = output / np.sum(output, axis=1, keepdims=True)  # Distribute relevance equally\n",
    "    return tf.convert_to_tensor(relevance, dtype=tf.float32)\n",
    "\n",
    "def compute_lrp(model, inputs, relevance_output):\n",
    "    \"\"\"\n",
    "    Compute Layer-Wise Relevance Propagation (LRP).\n",
    "    \"\"\"\n",
    "    relevance = tf.convert_to_tensor(relevance_output, dtype=tf.float32)\n",
    "\n",
    "    for layer in reversed(model.layers):\n",
    "        if isinstance(layer, tf.keras.layers.Dense):\n",
    "            weights, _ = layer.get_weights()\n",
    "            relevance = tf.matmul(relevance, tf.transpose(weights))\n",
    "        elif isinstance(layer, tf.keras.layers.Flatten):\n",
    "            relevance = tf.reshape(relevance, (-1, *layer.input_shape[1:]))\n",
    "        elif isinstance(layer, tf.keras.layers.Conv3D):\n",
    "            kernel, _ = layer.get_weights()\n",
    "            strides = [1, *layer.strides, 1]\n",
    "            padding = layer.padding.upper()\n",
    "            output_shape = [inputs.shape[0]] + list(layer.input_shape[1:])\n",
    "            relevance = tf.nn.conv3d_transpose(\n",
    "                relevance,\n",
    "                kernel,\n",
    "                output_shape=output_shape,\n",
    "                strides=strides,\n",
    "                padding=padding\n",
    "            )\n",
    "        elif isinstance(layer, tf.keras.layers.Dropout) or isinstance(layer, tf.keras.layers.InputLayer):\n",
    "            continue\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Unsupported layer type: {type(layer)}\")\n",
    "\n",
    "    return relevance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells = get_rows_Milan(5060, nr)\n",
    "length = 1780\n",
    "cluster_counts = np.load(os.path.join(cluster_directory, f'clusters_{city}_{k}.npy'))\n",
    "clustered_cells = {i: [] for i in range(k)}\n",
    "\n",
    "# Populate the cluster cells dictionary\n",
    "for idx, cluster_label in enumerate(cluster_counts):\n",
    "    cell_id = cells[idx]\n",
    "    clustered_cells[cluster_label].append(cell_id)\n",
    "\n",
    "results = []\n",
    "for cluster_label, cluster_cells in clustered_cells.items():\n",
    "    local_nr2 = nr2\n",
    "    local_num_cells = nr2 * nr2\n",
    "    total_cells = len(cluster_cells)\n",
    "\n",
    "    # Adjust if the number of cells exceeds available cells\n",
    "    if local_num_cells > total_cells:\n",
    "        largest_square = int(math.floor(math.sqrt(total_cells))) ** 2\n",
    "        local_num_cells = largest_square\n",
    "        local_nr2 = int(math.sqrt(local_num_cells))\n",
    "        print(f\"Number of cells for cluster {cluster_label} in K:{k} is set to {local_nr2} (nearest square number).\")\n",
    "    random.seed(42)\n",
    "    selected_cells = (random.sample(cluster_cells, local_num_cells) if random_flag else \n",
    "                        np.load(os.path.join(cluster_directory, f'closest_to_centroid/closest_bs_{city}_{k}_{cluster_label}_100.npy')).tolist()[:local_num_cells])\n",
    "\n",
    "    # Get the directory of the current script file\n",
    "    # config_path = f'config{local_nr2}.yml'\n",
    "    # with open(config_path, 'r') as file:\n",
    "    #     conf = yaml.safe_load(file)\n",
    "    # Prepare data\n",
    "    X_test, y_test, y_scalers = preprocess_cluster_data(selected_cells, cluster_cells, output_directory, city, lookback, local_nr2, data_type=\"test\")\n",
    "    X_test_original, y_test_original, y_scalers_original = preprocess_cluster_data(cells, cells, output_directory, city, lookback, nr, data_type=\"test\")\n",
    "\n",
    "    model_miMo_path = os.path.join(model_deepcog_miMo, f'random_selection_{random_flag}/{city}/k_{k}/cluster_{cluster_label}_size_{local_num_cells}.h5')\n",
    "    model_original_path = os.path.join(model_deepcog_original, f'{city}/deepcog_size_{nr}.h5')\n",
    "    model_miMo = load_model_deepcog(model_miMo_path)\n",
    "    predicted_miMo = model_miMo.predict(X_test)\n",
    "\n",
    "    model_original = load_model_deepcog(model_original_path)\n",
    "    predicted_original = model_original.predict(X_test_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass: Get predictions\n",
    "output = model_original(X_test_original)\n",
    "\n",
    "# Initialize relevance at the output layer (focus on class 0)\n",
    "relevance_output = np.zeros_like(output.numpy())\n",
    "target_class = 0  # Class to explain\n",
    "relevance_output[:, target_class] = output[:, target_class]\n",
    "\n",
    "# Compute LRP\n",
    "lrp_map = compute_lrp(model_original, X_test_original, relevance_output)\n",
    "\n",
    "# Print results\n",
    "print(\"LRP Map Shape:\", lrp_map.shape)  # Should match input shape (batch, lookback, size, size, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass to get predictions\n",
    "output = model_original(X_test_original)\n",
    "\n",
    "# Initialize relevance (focus on class 0)\n",
    "relevance_output = initialize_relevance(output.numpy(), target_class=0)\n",
    "\n",
    "# Compute LRP\n",
    "lrp_map = compute_lrp(model_original, X_test_original, relevance_output)\n",
    "\n",
    "# Print shape of relevance map\n",
    "print(\"LRP Map Shape:\", lrp_map.shape)  # Should match input shape (10, 3, 2, 2, 1)\n",
    "\n",
    "# Visualize relevance for the first sample\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Aggregate relevance over the temporal dimension (lookback)\n",
    "aggregated_lrp_map = np.sum(lrp_map.numpy(), axis=1)\n",
    "print(\"Aggregated LRP Map Shape:\", aggregated_lrp_map.shape)  # Should match input shape (10, 2, 2, 1)\n",
    "plt.imshow(aggregated_lrp_map[0, :, :, 0], cmap='hot', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.title(\"Relevance Map for First Sample\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the costs (deepcog cost and mae cost) for miMo and original deepcog and save them in a csv files\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "from functions.utils_lrp import *\n",
    "\n",
    "\n",
    "# Set multiprocessing start method\n",
    "multiprocessing.set_start_method('spawn', force=True)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"-1\"  # Set the GPU card to use\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Or '3' for FATAL logs only\n",
    "np.set_printoptions(suppress=True)\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for gpu_instance in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(gpu_instance, True)\n",
    "\n",
    "# Set global parameters\n",
    "lookback = 3\n",
    "nr = 21\n",
    "\n",
    "# Directories\n",
    "main_directory = '../../../../oracle-data/serly/Scalable_dnn/'\n",
    "dataset_directory = '../../../../oracle-data/serly/Scalable_dnn/PerBS/'\n",
    "cluster_directory = '../../../../oracle-data/serly/Scalable_dnn/cluster/'\n",
    "model_deepcog_mimo = '../../../../oracle-data/serly/Scalable_dnn/Trained_models/DeepCOG_MiMo_clusters/'\n",
    "output_directory = '../../../../oracle-data/serly/Scalable_dnn/MAE_plots/DeepCOG_miMo_vs_MiMo/'\n",
    "\n",
    "# Function to process a single parameter combination\n",
    "def process_combination(params):\n",
    "    K, nr2, city = params\n",
    "    \n",
    "    cells = get_rows_Milan(5060, nr)\n",
    "    cluster_counts = np.load(os.path.join(cluster_directory, f'clusters_{city}_{K}.npy'))\n",
    "    clustered_cells = {i: [] for i in range(K)}\n",
    "\n",
    "    # Populate the cluster cells dictionary\n",
    "    for idx, cluster_label in enumerate(cluster_counts):\n",
    "        cell_id = cells[idx]\n",
    "        clustered_cells[cluster_label].append(cell_id)\n",
    "\n",
    "    for cluster_label, cluster_cells in clustered_cells.items():\n",
    "        \n",
    "        # load the lrp scores\n",
    "        lrp_dir = os.path.join(main_directory, f'LRP_scores/mimo_model/{city}/K_{K}')\n",
    "        lrp_scores = np.load(os.path.join(lrp_dir, f'cluster_{cluster_label}_size_{total_cells}.npy'))\n",
    "\n",
    "\n",
    "        # Print results\n",
    "        print(\"LRP Map Shape:\", lrp_map.shape)  # Should match input shape (batch, lookback, size, size, 1)\n",
    "\n",
    "\n",
    "# Prepare combinations for multiprocessing\n",
    "Ks = [2, 3, 4, 5, 6, 10, 15, 20]\n",
    "nr2_values = [2, 3, 4, 5, 6, 7]\n",
    "cities = ['Milan']\n",
    "combinations = [(K, nr2, city) for K in Ks for nr2 in nr2_values for city in cities]\n",
    "\n",
    "# Run multiprocessing\n",
    "if __name__ == '__main__':\n",
    "    with Pool(100) as p:\n",
    "        all_results = p.map(process_combination, combinations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_lrp_scores(Ks, city, lookback=3, nr=21):\n",
    "    \"\"\"\n",
    "    Process LRP scores for a specific city and multiple cluster counts (Ks).\n",
    "    \n",
    "    Args:\n",
    "        Ks (list[int]): List of cluster counts to process.\n",
    "        city (str): City name ('EUMA' or 'Milan').\n",
    "        lookback (int): Lookback window for the model (default: 3).\n",
    "        nr (int): Number of rows/columns in the grid (default: 21).\n",
    "    \"\"\"\n",
    "    # Directories\n",
    "    main_directory = '../../../../../oracle-data/serly/Scalable_dnn/'\n",
    "    dataset_directory = f'{main_directory}PerBS/'\n",
    "    cluster_directory = f'{main_directory}cluster/'\n",
    "    model_deepcog_mimo = f'{main_directory}Trained_models/DeepCOG_MiMo_clusters/'\n",
    "\n",
    "    cells = get_rows_Milan(5060, nr)\n",
    "\n",
    "    for K in Ks:\n",
    "        print(f\"Processing city: {city}, K = {K}\")\n",
    "        \n",
    "        output_directory = f'{model_deepcog_mimo}{city}/k_{K}/'\n",
    "        lrp_dir = os.path.join(main_directory, f'LRP_scores/mimo_model/{city}/K_{K}')\n",
    "\n",
    "        # Load the cluster assignments\n",
    "        cluster_counts = np.load(cluster_directory + f'clusters_{city}_{K}.npy')\n",
    "\n",
    "        # Initialize a dictionary to store the cell IDs for each cluster\n",
    "        clustered_cells = {i: [] for i in range(K)}\n",
    "\n",
    "        # Categorize the cells into clusters\n",
    "        for idx, cluster_label in enumerate(cluster_counts):\n",
    "            cell_id = cells[idx]\n",
    "            clustered_cells[cluster_label].append(cell_id)\n",
    "\n",
    "        # Process each cluster\n",
    "        for cluster_label in clustered_cells:\n",
    "            # Get the cell IDs for the current cluster\n",
    "            cluster_cells = clustered_cells.get(cluster_label, [])\n",
    "            if not cluster_cells:\n",
    "                print(f\"No cells found for Cluster {cluster_label}\")\n",
    "                continue\n",
    "\n",
    "            total_cells = len(cluster_cells)  # Total number of cells in this cluster\n",
    "            lrp_scores_path = os.path.join(lrp_dir, f'cluster_{cluster_label}_size_{total_cells}.npy')\n",
    "            cell_ids_path = os.path.join(output_directory, f'cluster_cells_{cluster_label}.npy')\n",
    "\n",
    "            if not os.path.exists(lrp_scores_path) or not os.path.exists(cell_ids_path):\n",
    "                print(f\"LRP scores or cell IDs missing for Cluster {cluster_label}\")\n",
    "                continue\n",
    "\n",
    "            # Load LRP scores and cell IDs\n",
    "            lrp_scores = np.load(lrp_scores_path)\n",
    "            print(lrp_scores.shape)\n",
    "            cell_ids = np.load(cell_ids_path)\n",
    "\n",
    "\n",
    "            # Process LRP scores\n",
    "            # remove the 4-dimension only\n",
    "            my_scores = np.squeeze(lrp_scores, axis=-1)\n",
    "            my_scores = my_scores[:, 2, :, :]  # Extract specific time step dimension\n",
    "            my_scores = np.mean(my_scores, axis=0)  # Average over time\n",
    "            my_scores = my_scores.flatten()  # Flatten into 1D array\n",
    "\n",
    "            # Sort cells in descending order of scores\n",
    "            sorted_indices = np.argsort(my_scores)[::-1]\n",
    "            cells_sorted = cell_ids[sorted_indices]\n",
    "\n",
    "            # Save the sorted cells\n",
    "            sorted_path = os.path.join(lrp_dir, f'sorted_cells_{cluster_label}.npy')\n",
    "            np.save(sorted_path, cells_sorted)\n",
    "\n",
    "\n",
    "Ks = [2, 3, 4, 5, 6, 10, 15, 20]\n",
    "process_lrp_scores(Ks=Ks, city='Milan')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
